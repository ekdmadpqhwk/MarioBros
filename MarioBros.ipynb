{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1HvRNphWqHeCtemUYHPp8A5mWVpOi5Q5o","timestamp":1691497118361}],"toc_visible":true,"authorship_tag":"ABX9TyNR5ZpBIDcQOOMFO4NN4+jA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Mario Bros RL implementation\n","Sumin Park /\n","2023.08.01\n","## Implementation details\n","* MARL\n","* Implement Sarsa/Q-learning/Expected Sarsa (maybe all three, if it's easy to implement)\n","* Deep Q-Learning?\n","* Make a graph comparing the performances of each method\n","\n","## Need to learn\n","* How having 2 agents affect Q-**learning**"],"metadata":{"id":"P7lBlRx0mEWu"}},{"cell_type":"markdown","source":["# Plan here\n","## What I know\n","* Mixed-sum game of planning and control\n","* Goal of the game: kick the pest off the step\n","** This requires 2 steps\n","> 1. Hit the floor beneath the pest, which knocks the pest onto its back.\n","> 2. Move up to the floor and kick it off. (+800 reward)\n","\n","## Game Plan\n","* Since there are only 2 agents in this game, there is no need to implement mean-field Q-learning\n","* Use of joint action space (18 X 18 for 2 mario bros)\n","* Epsilon-greedy policy/what alpha value? pretty large\n","* Q-network (add target network after testing Q-network)\n","* Replay memory\n","* Preprocessing of the frames (how many frames to stack together as the state?)\n","\n","\n","## Code in parts\n","### 1. Initialize replay memory\n",">Initially, the game will run up to N frames while storing exp variables to D. After it reaches N, replace D = D[1:N] (or use deque?)\n","\n","* Variables: exp (e:s, a, r, s'); replay memory pool (D: list of N # of e); N (capacity); batch (batch size to sample from D at each training - updating Q-network)\n","\n","### 2. Build model\n","> Build a Keras model including a few convolutional layers.\n","\n","* Variables: state_size (state space); action_size (joint action space)\n","\n","### 3. Train a model\n","> Train the model with target and current q values. *Important*: For actions that were chosen, target comes from Bellman eq. For actions not chosen, target is just predicted values from our q-network (in lunar example, however in cartpole it seems that the target for unchosen action is 0 ...). Each agent will have seperate q-network to be trained on, since q-values of the agents should be different on the same state.\n","\n","* Variables: current and future state; reward; termination values (termination & truncation); lr (learning rate); optimizer;\n","\n","\n","* Functions: get_q_target(given current reward, argmax_q_value (from next obs/reward), alpha, return a target to calculate loss).\n","\n","### 4. Play game\n","\n","> Loop through each frame of the game.\n","\n","* Variables: epsilon (starting e, min e, decay_rate), episode (# of episode to train the model); T (# of frames per episode); total (total reward for each agent)\n","\n","* Functions: get_action (given current state and e values, return action according to e greedy policy); store_memory (given current obs, add it replay memory)\n","\n","\n","### 5. Testing the q-network\n","> Not sure if this is necessary? By the end of training a model the\n","\n","\n","### 6. Handling 2 agents\n","> Q-network determines the highest joint action value. Need functions that will convert this value to individual actions. Joint_action_space is of shape flattened(18, 18) + 2*18, the last of which determines an action taken by one agent when the other is dead.\n","\n","* Functions: joint_action_to_actions (given two action values, convert it to a joint action value in join_action_space (18*18 +\n","\n","### 7. Google drive\n","> It might be necessary to save q-network param inside a file on google drive, so as not to have to run everything all over agian.\n","\n","\n","## Loops\n","\n","## Remaining questions\n","* When sampling from the memory pool, do I sample a stack of frames to use as samples together? -> most likely yes\n","* ~~How do I take in a stack of sequential frames through input node?~~ **DONE**\n","* How do I use multiagent wrappers? (from supersuit)\n","* ~~What happens to the action space when one agent is dead?~~\n","*\n","\n","## Useful bits of code\n","```Python\n","from collections import deque\n","self.memory = deque(maxlen=N)\n","self.memory.appendleft() # or something like this\n","```"],"metadata":{"id":"IzgM-z6NZAIU"}},{"cell_type":"markdown","source":["# Mount Goolge Drive"],"metadata":{"id":"EVlS3wJ5omuR"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flzfVoDbopBv","executionInfo":{"status":"ok","timestamp":1691496167204,"user_tz":-540,"elapsed":29850,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"49934270-f323-467f-f499-c3c24a812155"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","\n","%cd /content/drive/MyDrive/Github/MarioBros\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fczUvNgcpTzq","executionInfo":{"status":"ok","timestamp":1691496949791,"user_tz":-540,"elapsed":391,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"b2f2ca91-9dd1-4b3c-9ed0-ea93319b6f49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/MarioBros\n","/content/drive/MyDrive/Github/MarioBros\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Lv7J9U8NEN7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pettingzoo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcngKu3MoUXO","executionInfo":{"status":"ok","timestamp":1691484038156,"user_tz":-540,"elapsed":5095,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"0e19280e-0d01-4d4c-f1ad-6d9cb5bc4d48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pettingzoo in /usr/local/lib/python3.10/dist-packages (1.23.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo) (1.22.4)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo) (0.29.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (4.7.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (0.0.4)\n"]}]},{"cell_type":"code","source":["!pip install pettingzoo[atari]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GIFVLAdpoesd","executionInfo":{"status":"ok","timestamp":1691484047790,"user_tz":-540,"elapsed":9639,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"b00db386-3d72-4ad9-c851-5e628423d98b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pettingzoo[atari] in /usr/local/lib/python3.10/dist-packages (1.23.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[atari]) (1.22.4)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[atari]) (0.29.0)\n","Requirement already satisfied: multi-agent-ale-py==0.1.11 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[atari]) (0.1.11)\n","Requirement already satisfied: pygame==2.3.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[atari]) (2.3.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (4.7.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (0.0.4)\n"]}]},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hE5c2OlSwion","executionInfo":{"status":"ok","timestamp":1691484054944,"user_tz":-540,"elapsed":7170,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"3f77bd44-8671-4a9a-8f17-aba728e0c1d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.13)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"]}]},{"cell_type":"code","source":["!pip install gymnasium[accept-rom-license]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42baOqV5xSA4","executionInfo":{"status":"ok","timestamp":1691484063757,"user_tz":-540,"elapsed":8817,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"8b4aa8a8-07a7-49a0-e882-78a23bb87645"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.22.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.7.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n","Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.4.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.65.0)\n","Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.6.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.4)\n"]}]},{"cell_type":"code","source":["%pip install -U gym>=0.26.2\n","%pip install -U gym[atari,accept-rom-license]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KnWor3UgQie8","executionInfo":{"status":"ok","timestamp":1691484084886,"user_tz":-540,"elapsed":21146,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"2c08e0b2-8e4c-43a0-d24d-a64b96d4b4fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (1.22.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n","Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n","Requirement already satisfied: ale-py~=0.8.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.8.1)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (6.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (4.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.65.0)\n","Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.6.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.4)\n"]}]},{"cell_type":"code","source":["!AutoROM"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lr2rBsvExn6r","executionInfo":{"status":"ok","timestamp":1691484174615,"user_tz":-540,"elapsed":89758,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"258dce64-5000-4d36-8dbd-287ec14e7e02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AutoROM will download the Atari 2600 ROMs.\n","They will be installed to:\n","\t/usr/local/lib/python3.10/dist-packages/AutoROM/roms\n","\t/usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms\n","\n","Existing ROMs will be overwritten.\n","\n","I own a license to these Atari 2600 ROMs.\n","I agree to not distribute these ROMs and wish to proceed: [Y/n]: \n","Aborted!\n"]}]},{"cell_type":"code","source":["!pip install supersuit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSOLmkSW82iv","executionInfo":{"status":"ok","timestamp":1691484180789,"user_tz":-540,"elapsed":6178,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"51849886-6fbb-4372-f8b6-3255c06bbbc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: supersuit in /usr/local/lib/python3.10/dist-packages (3.9.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.22.4)\n","Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.29.0)\n","Requirement already satisfied: tinyscaler>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.2.6)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.7.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n"]}]},{"cell_type":"code","source":["# Fool Colab that it has a video card\n","!pip install pygame\n","\n","import os\n","os.environ['SDL_VIDEODRIVER']='dummy'\n","import pygame\n","pygame.display.set_mode((640,480))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VaftW8EZ0BL5","executionInfo":{"status":"ok","timestamp":1691487938393,"user_tz":-540,"elapsed":5249,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"4f585b3d-7b88-475d-f6f6-431373099503"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.3.0)\n"]},{"output_type":"execute_result","data":{"text/plain":["<Surface(640x480x32 SW)>"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["# Import packages"],"metadata":{"id":"DrK1P1Ca_-lk"}},{"cell_type":"code","source":["from pettingzoo.atari import mario_bros_v3\n","\n","import numpy as np\n","import numpy.random as npr\n","\n","import supersuit\n","\n","import tensorflow as tf\n","from tensorflow.keras import optimizers, losses\n","from tensorflow.keras import Model\n","\n","from collections import deque\n","from tqdm import tqdm\n","\n","# from Ipython.display import clear_output"],"metadata":{"id":"1e09Zd4Elm3F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create environment and preprocess"],"metadata":{"id":"iQxmMz8_0HCi"}},{"cell_type":"code","source":["'''\n","Create a custom environment that includes a joint action space\n","'''\n","from pettingzoo.atari.base_atari_env import ParallelAtariEnv\n","\n","class jointAtariEnv(ParallelAtariEnv):\n","  super().__init__()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"uJeORPEGB1x7","executionInfo":{"status":"error","timestamp":1691484181723,"user_tz":-540,"elapsed":939,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"9852b1fa-af89-4c16-df82-93da897a0cde"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-e3889dd0a9cc>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpettingzoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_atari_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelAtariEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mjointAtariEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallelAtariEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-e3889dd0a9cc>\u001b[0m in \u001b[0;36mjointAtariEnv\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mjointAtariEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallelAtariEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: super(): no arguments"]}]},{"cell_type":"code","source":["env = mario_bros_v3.env(render_mode=\"human\", full_action_space=True)\n","\n","stacked_frames = 4\n","\n","# maxes over the last 2 frames to deal with frame flickering\n","env = supersuit.max_observation_v0(env, 2)\n","\n","# repeat_action_probability is set to 0.25 to introduce non-determinism to the system\n","env = supersuit.sticky_actions_v0(env, repeat_action_probability=0.25)\n","\n","# skip frames for faster processing and less control\n","# to be compatible with gym, use frame_skip(env, (2,5))\n","env = supersuit.frame_skip_v0(env, 4)\n","\n","# downscale observation for faster processing\n","env = supersuit.resize_v1(env, 84, 84) # not sure if the x_ and y_size are good\n","\n","# stack frames together to give more info on what is happening at one timestep\n","env = supersuit.frame_stack_v1(env, stacked_frames)\n","\n","# preprocessing for MADRL\n","env = supersuit.agent_indicator_v0(env)\n","\n","env = supersuit.pad_observations_v0(env) # Is this necessary? The obs will be the same for both agents.\n","\n"],"metadata":{"id":"ALJHl1-N0NsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_env = env.unwrapped\n","print(type(raw_env))\n","print(raw_env.full_action_space)\n","print(raw_env.action_mapping)\n","# So now what I have to do is modify my env... and map the action to 18 * 18 ...\n","# but how does that actually change the next state of the game!!! where in the class\n","# does the action_mapping work\n","env.reset()\n","print(env.last())"],"metadata":{"id":"d4tReNKQxtpx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define Q-network model"],"metadata":{"id":"sK82py2y1tqd"}},{"cell_type":"code","source":["state_size = env.observation_space('first_0').shape # convert to np array?\n","action_size = env.action_space('first_0').n\n","\n","batch_size = 100\n","\n","# hyper parameters\n","lr = 1\n","gamma = 0\n","\n","print(state_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7E9L3oQiQWMc","executionInfo":{"status":"ok","timestamp":1691484209469,"user_tz":-540,"elapsed":320,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"cc10b182-def6-4caa-9fe4-ab58671dbdf5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(84, 84, 14)\n"]}]},{"cell_type":"code","source":["class MarioBrosQNet(tf.keras.Model):\n","  def __init__(self, state_size, action_size):\n","\n","    super(MarioBrosQNet, self).__init__()\n","\n","    self.conv1 = tf.keras.layers.Conv2D(32,\n","                                        kernel_size=(8, 8), strides=(4, 4),\n","                                        activation='relu',\n","                                        data_format='channels_last',\n","                                        input_shape=state_size)\n","\n","    self.maxpool1 = tf.keras.layers.MaxPooling2D(pool_size=(4, 4),\n","                                                 strides=(4, 4),\n","                                                 data_format='channels_last')\n","    self.conv2 = tf.keras.layers.Conv2D(64,\n","                                        kernel_size=(3, 3), strides=(2, 2),\n","                                        activation='relu',\n","                                        data_format='channels_last')\n","    self.maxpool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n","                                                 strides=(2, 2),\n","                                                 data_format='channels_last')\n","\n","    self.flatten = tf.keras.layers.Flatten()\n","    self.dense1 = tf.keras.layers.Dense(512,\n","                                        activation='relu')\n","    self.dense2 = tf.keras.layers.Dense(256,\n","                                        activation='relu')\n","\n","    self.value = tf.keras.layers.Dense(action_size,\n","                                        activation='linear')\n","\n","\n","  def call(self, state):\n","    conv1 = self.conv1(state)\n","    maxpool1 = self.maxpool1(conv1)\n","    conv2 = self.conv2(maxpool1)\n","    maxpool2 = self.maxpool2(conv2)\n","    flatten = self.flatten(maxpool2)\n","    dense1 = self.dense1(flatten)\n","    dense2 = self.dense2(dense1)\n","    value = self.value(dense2)\n"],"metadata":{"id":"85jvwWmW1yql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize Q-Networks for 2 agents\n","\n","qNet_1 = MarioBrosQNet(state_size, action_size)\n","qNet_2 = MarioBrosQNet(state_size, action_size)"],"metadata":{"id":"RLORmlrronfb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test q-network moel\n","\n","env.reset()\n","print(env.last()[0].dtype)\n","sample_data = np.array([env.last()[0], env.last()[0]])\n","#tf.cast(sample_data, dtype=tf.float32) # state data need to be recast into float32 (from uint8) to be compaitable with conv2d layer\n","# or why is it not necessary???\n","print(sample_data.dtype)\n","\n","qNet_1.build(sample_data.shape) # build the model with a sample data\n","qNet_1.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RYbglQlcUvF4","executionInfo":{"status":"ok","timestamp":1691490482348,"user_tz":-540,"elapsed":329,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"bd21a3b7-980f-45d0-cadc-99bbc3c0e47c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["uint8\n","uint8\n","Model: \"mario_bros_q_net_35\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_69 (Conv2D)          multiple                  28704     \n","                                                                 \n"," max_pooling2d_68 (MaxPoolin  multiple                 0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_70 (Conv2D)          multiple                  18496     \n","                                                                 \n"," max_pooling2d_69 (MaxPoolin  multiple                 0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_34 (Flatten)        multiple                  0         \n","                                                                 \n"," dense_102 (Dense)           multiple                  33280     \n","                                                                 \n"," dense_103 (Dense)           multiple                  131328    \n","                                                                 \n"," dense_104 (Dense)           multiple                  4626      \n","                                                                 \n","=================================================================\n","Total params: 216,434\n","Trainable params: 216,434\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["# Train q-nets"],"metadata":{"id":"n6nrCnHX4MvW"}},{"cell_type":"code","source":["def train_model(q_net, replay_memory, batch_size, lr):\n","  pass"],"metadata":{"id":"GZqXDXrH4Q-f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define policy"],"metadata":{"id":"geVpY06MpzTh"}},{"cell_type":"code","source":["# initial epsilon, decay rate, final epsilon values\n","epsilon = 1\n","max_epsilon = 1\n","decay_rate = 0.005\n","min_epsilon = 0.05\n","\n","def get_next_action(state, epsilon, agent):\n","  if (npr.rand() < epsilon):\n","    action = env.action_space(agent).sample()\n","  else:\n","    if agent == 'first_0':\n","      action = np.argmax(qNet_1.predict(state,\n","                                        verbose = 0))\n","    elif agent == 'second_0':\n","      action = np.argmax(qNet_2.predict(state,\n","                                        verbose = 0))\n","\n","  return action"],"metadata":{"id":"ZZwP3_ZuOTWz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run the game"],"metadata":{"id":"S7J0eWh4wnq-"}},{"cell_type":"code","source":["# episode parameters\n","\n","n_games = 1\n","\n","max_frames = 11\n","\n","for i in range(n_games):\n","\n","  env.reset()\n","\n","  # keep a seperate replay memory for each agent\n","  replay_memory = {'first_0': deque(maxlen=100000),\n","                   'second_0': deque(maxlen=100000)}\n","\n","  # empty dictionary to store total rewards for each game\n","  episode_reward = {'first_0': 0,\n","                    'second_0': 0}\n","\n","  done = False\n","  j = 0\n","\n","  while not done:\n","\n","    if j >= max_frames:\n","      break\n","\n","    # I only need one state? no because after one agent plays the state changes...\n","    for agent in env.agent_iter():\n","\n","        state, _, done, trunc, _ = env.last() # last reward doesn't matter\n","        action = get_next_action(state, epsilon, agent)\n","        env.step(action)\n","\n","        #env.render()\n","\n","        next_state, reward, done, trunc, _ = env.last()\n","\n","        episode_reward[agent] += reward\n","\n","        replay_memory[agent].append([state, action, reward, next_state, done])\n","\n","\n","    if ((j % 10) == 0):\n","      train_model(replay_memory, batch_size, lr)\n","\n","    j += 1\n"],"metadata":{"id":"eJNtho1Vwp0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Code testing"],"metadata":{"id":"WncuB_MGs2Jc"}},{"cell_type":"code","source":["from collections import deque\n","\n","memories = {'one': deque(maxlen=8),\n","            'two': deque(maxlen=8)}\n","agents = ['one', 'two']\n","\n","for agent in agents:\n","  for i in range(8):\n","    memories[agent].append(i)\n","\n","def test(dict):\n","  print(\"original: \", dict)\n","  print(\"modifying\")\n","  dict['one'][0] = 1\n","  print(\"after: \", dict)\n","\n","test(memories)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4AQamYGIg9p","executionInfo":{"status":"ok","timestamp":1691490666851,"user_tz":-540,"elapsed":344,"user":{"displayName":"Sumin Park","userId":"01077014681694279216"}},"outputId":"c2b23f1d-fd9c-4602-b526-f0153dcffd51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["original:  {'one': deque([0, 1, 2, 3, 4, 5, 6, 7], maxlen=8), 'two': deque([0, 1, 2, 3, 4, 5, 6, 7], maxlen=8)}\n","modifying\n","after:  {'one': deque([1, 1, 2, 3, 4, 5, 6, 7], maxlen=8), 'two': deque([0, 1, 2, 3, 4, 5, 6, 7], maxlen=8)}\n"]}]}]}